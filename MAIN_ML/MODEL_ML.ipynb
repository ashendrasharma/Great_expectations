{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be84613e-7358-4ec0-b3e8-2fee3a219e52",
   "metadata": {},
   "source": [
    "# TRAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf11135f-7d61-4205-be26-50a2a669fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5d728-26ef-4aa2-93db-ca0b9459b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('LINK')\n",
    "db = client['intelliinvest']\n",
    "fundamentals_collection = db['STOCK_FUNDAMENTALS']\n",
    "signals_collection = db['STOCK_SIGNALS_COMPONENTS_10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5f095-501d-4055-b0ec-a02ac7f94360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all securityIds\n",
    "fundamentals_security_ids = fundamentals_collection.distinct('securityId')\n",
    "signals_security_ids = signals_collection.distinct('securityId')\n",
    "all_security_ids = set(fundamentals_security_ids).intersection(set(signals_security_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1828a7f3-45ab-40d9-9e5a-19b05cabbce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each securityId\n",
    "for security_id in all_security_ids:\n",
    "    query = {\"securityId\": security_id}\n",
    "    fundamentals_results = fundamentals_collection.find(query)\n",
    "    signals_results = signals_collection.find(query)\n",
    "\n",
    "    fundamentals_df = pd.DataFrame(list(fundamentals_results))\n",
    "    signals_df = pd.DataFrame(list(signals_results))\n",
    "\n",
    "    fundamentals_df = fundamentals_df.rename(columns={'todayDate': 'signalDate'})\n",
    "    fundamentals_df['signalDate'] = pd.to_datetime(fundamentals_df['signalDate'])\n",
    "    signals_df['signalDate'] = pd.to_datetime(signals_df['signalDate'])\n",
    "    \n",
    "    merged_data = pd.merge(fundamentals_df, signals_df, on=['securityId', 'signalDate'], how='inner')\n",
    "\n",
    "    selected_columns = ['signalDate', 'closePrice', 'securityId', 'alMarketCap', 'stdDevOfReturn', 'TRn', 'ADXn', 'high10Day', 'low10Day', 'stochastic10Day', 'range10Day', 'percentKFlow', 'percentDFlow', 'upperBound', 'lowerBound', 'bandwidth', 'movingAverage_5', 'movingAverage_10', 'movingAverage_15', 'movingAverage_25', 'movingAverage_50']\n",
    "    df = merged_data[selected_columns]\n",
    "\n",
    "    df.set_index('securityId', inplace=True)\n",
    "\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].dt.date\n",
    "    df.loc[:, 'signalDate'] = pd.to_datetime(df['signalDate'])\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].astype(str)\n",
    "    \n",
    "    df = df.sort_values(by='signalDate')\n",
    "\n",
    "    df['cap_category'] = pd.cut(df['alMarketCap'], bins=[-np.inf, df['alMarketCap'].median(), np.inf], labels=['Low Midcap', 'High Midcap'])\n",
    "    df['std_category'] = pd.cut(df['stdDevOfReturn'], bins=[-np.inf, df['stdDevOfReturn'].median(), np.inf], labels=['Low STD', 'High STD'])\n",
    "    df['category'] = df['cap_category'].astype(str) + ', ' + df['std_category'].astype(str)\n",
    "    \n",
    "    train_df = df[df['signalDate'] < df['signalDate'].max()]\n",
    "    \n",
    "    if len(train_df) < 3:\n",
    "        print(f\"Not enough data for securityId: {security_id}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    X_train = train_df.drop(columns=['closePrice', 'signalDate'])\n",
    "    y_train = train_df['closePrice']\n",
    "    \n",
    "    param_grid_lr = {\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    param_grid_dt = {\n",
    "        'base_estimator__max_depth': [10, 20],\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    param_grid_gb = {\n",
    "        'base_estimator__n_estimators': [50, 100],\n",
    "        'base_estimator__learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    grid_lr = GridSearchCV(estimator=BaggingRegressor(base_estimator=Ridge(), random_state=42),\n",
    "                           param_grid=param_grid_lr, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_lr.fit(X_train, y_train)\n",
    "    \n",
    "    grid_dt = GridSearchCV(estimator=BaggingRegressor(base_estimator=DecisionTreeRegressor(), random_state=42),\n",
    "                           param_grid=param_grid_dt, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    \n",
    "    grid_gb = GridSearchCV(estimator=BaggingRegressor(base_estimator=GradientBoostingRegressor(), random_state=42),\n",
    "                           param_grid=param_grid_gb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_gb.fit(X_train, y_train)\n",
    "    \n",
    "    with open(f'models/lr_model_{security_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_lr.best_estimator_, f)\n",
    "    with open(f'models/dt_model_{security_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_dt.best_estimator_, f)\n",
    "    with open(f'models/gb_model_{security_id}.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_gb.best_estimator_, f)\n",
    "    \n",
    "    print(f\"Models trained and saved for securityId: {security_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43beae7c-4805-4a94-9036-cd105b14fcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d607b085-12cb-40b4-8841-14ba8e954481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d0244b-7ba6-40d1-8323-b24248176a61",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c94b9-0d10-40e7-981e-45b0e6b61933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f764f-7b44-414d-9d95-396a436a8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2ffad-719e-474f-84ef-c2ba4f9e2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('LINK')\n",
    "db = client['intelliinvest']\n",
    "fundamentals_collection = db['STOCK_FUNDAMENTALS']\n",
    "signals_collection = db['STOCK_SIGNALS_COMPONENTS_10']\n",
    "predictions_collection = db['STOCK_PREDICTIONS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95a7e0-d4e8-4c68-9b0d-3e06e3451ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE\n",
    "def calculate_rmse(true_values, predictions):\n",
    "    return np.sqrt(mean_squared_error(true_values, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7247fb4-c316-431f-a11f-9c9fd2c70729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentage error\n",
    "def calculate_percentage_error(true_values, predictions):\n",
    "    return np.abs((true_values - predictions) / true_values) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbebde8-2aa0-41b5-977a-cb02a068a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all securityIds\n",
    "fundamentals_security_ids = fundamentals_collection.distinct('securityId')\n",
    "signals_security_ids = signals_collection.distinct('securityId')\n",
    "all_security_ids = set(fundamentals_security_ids).intersection(set(signals_security_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3cf059-c25f-4c18-9f6b-1b2367f065ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each securityId\n",
    "for security_id in all_security_ids:\n",
    "    query = {\"securityId\": security_id}\n",
    "    fundamentals_results = fundamentals_collection.find(query)\n",
    "    signals_results = signals_collection.find(query)\n",
    "\n",
    "    fundamentals_df = pd.DataFrame(list(fundamentals_results))\n",
    "    signals_df = pd.DataFrame(list(signals_results))\n",
    "\n",
    "    fundamentals_df = fundamentals_df.rename(columns={'todayDate': 'signalDate'})\n",
    "    fundamentals_df['signalDate'] = pd.to_datetime(fundamentals_df['signalDate'])\n",
    "    signals_df['signalDate'] = pd.to_datetime(signals_df['signalDate'])\n",
    "    \n",
    "    merged_data = pd.merge(fundamentals_df, signals_df, on=['securityId', 'signalDate'], how='inner')\n",
    "\n",
    "    selected_columns = ['signalDate', 'closePrice', 'securityId', 'alMarketCap', 'stdDevOfReturn', 'TRn', 'ADXn', 'high10Day', 'low10Day', 'stochastic10Day', 'range10Day', 'percentKFlow', 'percentDFlow', 'upperBound', 'lowerBound', 'bandwidth', 'movingAverage_5', 'movingAverage_10', 'movingAverage_15', 'movingAverage_25', 'movingAverage_50']\n",
    "    df = merged_data[selected_columns]\n",
    "\n",
    "    df.set_index('securityId', inplace=True)\n",
    "\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].dt.date\n",
    "    df.loc[:, 'signalDate'] = pd.to_datetime(df['signalDate'])\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].astype(str)\n",
    "    \n",
    "    df = df.sort_values(by='signalDate')\n",
    "\n",
    "    df['cap_category'] = pd.cut(df['alMarketCap'], bins=[-np.inf, df['alMarketCap'].median(), np.inf], labels=['Low Midcap', 'High Midcap'])\n",
    "    df['std_category'] = pd.cut(df['stdDevOfReturn'], bins=[-np.inf, df['stdDevOfReturn'].median(), np.inf], labels=['Low STD', 'High STD'])\n",
    "    df['category'] = df['cap_category'].astype(str) + ', ' + df['std_category'].astype(str)\n",
    "\n",
    "    if len(df) < 3:\n",
    "        print(f\"Not enough data for securityId: {security_id}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    X = df.drop(columns=['closePrice', 'signalDate'])\n",
    "    y = df['closePrice']\n",
    "\n",
    "    try:\n",
    "        with open(f'models/lr_model_{security_id}.pkl', 'rb') as f:\n",
    "            lr_model = pickle.load(f)\n",
    "        with open(f'models/dt_model_{security_id}.pkl', 'rb') as f:\n",
    "            dt_model = pickle.load(f)\n",
    "        with open(f'models/gb_model_{security_id}.pkl', 'rb') as f:\n",
    "            gb_model = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model files not found for securityId: {security_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    periods = {\n",
    "        'daily': 1,\n",
    "        'weekly': 7,\n",
    "        'monthly': 30,\n",
    "        'quarterly': 90\n",
    "    }\n",
    "\n",
    "    last_date = pd.to_datetime(df['signalDate'].max())\n",
    "\n",
    "    for period, days in periods.items():\n",
    "        date = last_date + timedelta(days=days)\n",
    "        new_data = df.iloc[-1:].copy()\n",
    "        new_data['signalDate'] = date\n",
    "        new_data = new_data.drop(columns=['closePrice'])\n",
    "\n",
    "        lr_predictions = lr_model.predict(new_data.drop(columns=['signalDate']))\n",
    "        dt_predictions = dt_model.predict(new_data.drop(columns=['signalDate']))\n",
    "        gb_predictions = gb_model.predict(new_data.drop(columns=['signalDate']))\n",
    "\n",
    "        prediction_doc = {\n",
    "            'securityId': security_id,\n",
    "            'signalDate': date,\n",
    "            'period': period,\n",
    "            'Bagging_LR_Prediction': lr_predictions[0],\n",
    "            'Bagging_DT_Prediction': dt_predictions[0],\n",
    "            'Bagging_GB_Prediction': gb_predictions[0],\n",
    "            'Actual_Price': df['closePrice'].iloc[-1],\n",
    "            'Bagging_LR_Percentage_Error': calculate_percentage_error(df['closePrice'].iloc[-1], lr_predictions[0]),\n",
    "            'Bagging_DT_Percentage_Error': calculate_percentage_error(df['closePrice'].iloc[-1], dt_predictions[0]),\n",
    "            'Bagging_GB_Percentage_Error': calculate_percentage_error(df['closePrice'].iloc[-1], gb_predictions[0]),\n",
    "            'Data_Category': df['category'].iloc[-1]\n",
    "        }\n",
    "        predictions_collection.insert_one(prediction_doc)\n",
    "        print(f\"Predictions stored for securityId: {security_id}, date: {date}, period: {period}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a6a81-ea82-41c0-b30e-cfc7b081b993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c08fc-e4ca-4ce9-82d8-84bef156fe80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d00d69-e6cf-4aca-8a09-a10ffe3efdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14283fdf-8538-4e1e-9ed2-23ab56c8d7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551d9a7-7c6b-4054-a720-4123ccca8df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49742def-0f00-4627-82d7-496def975b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231c8b1-30a5-410e-91ea-f3995d65a441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
