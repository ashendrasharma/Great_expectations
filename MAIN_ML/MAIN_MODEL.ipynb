{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0310a5-0ae5-4f88-8508-cf4f7d4bf85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e623b7-47d0-4b5c-880e-32270a7d9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://intelliinvest:intelliinvest@67.211.219.52:27017/intelliinvest')\n",
    "db = client['intelliinvest']\n",
    "fundamentals_collection = db['STOCK_FUNDAMENTALS']\n",
    "signals_collection = db['STOCK_SIGNALS_COMPONENTS_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddcc7f-37cc-4763-9a0a-985cb2650f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE\n",
    "def calculate_rmse(true_values, predictions):\n",
    "    return np.sqrt(mean_squared_error(true_values, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e15c98-039c-443e-8f71-293e7737d18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate percentage error\n",
    "def calculate_percentage_error(true_values, predictions):\n",
    "    return np.abs((true_values - predictions) / true_values) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d257069-28c2-48ce-a432-f3f74ed76849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving all securityIds\n",
    "fundamentals_security_ids = fundamentals_collection.distinct('securityId')\n",
    "signals_security_ids = signals_collection.distinct('securityId')\n",
    "all_security_ids = set(fundamentals_security_ids).intersection(set(signals_security_ids))\n",
    "\n",
    "# Placeholder for results\n",
    "all_results = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f875c82-62af-401d-b2fa-dbe6854dc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each securityId\n",
    "for security_id in all_security_ids:\n",
    "    # Query data for the current securityId\n",
    "    query = {\"securityId\": security_id}\n",
    "    fundamentals_results = fundamentals_collection.find(query).limit(100000000000)\n",
    "    signals_results = signals_collection.find(query).limit(100000000000)\n",
    "\n",
    "    fundamentals_df = pd.DataFrame(list(fundamentals_results))\n",
    "    signals_df = pd.DataFrame(list(signals_results))\n",
    "\n",
    "    # Renaming columns and merge datasets\n",
    "    fundamentals_df = fundamentals_df.rename(columns={'todayDate': 'signalDate'})\n",
    "    fundamentals_df['signalDate'] = pd.to_datetime(fundamentals_df['signalDate'])\n",
    "    signals_df['signalDate'] = pd.to_datetime(signals_df['signalDate'])\n",
    "    \n",
    "    merged_data = pd.merge(fundamentals_df, signals_df, on=['securityId', 'signalDate'], how='inner')\n",
    "\n",
    "    selected_columns = ['signalDate', 'closePrice', 'securityId', 'TRn', 'ADXn', 'high10Day', 'low10Day', 'stochastic10Day', 'range10Day', 'percentKFlow', 'percentDFlow', 'upperBound', 'lowerBound', 'bandwidth', 'movingAverage_5', 'movingAverage_10', 'movingAverage_15', 'movingAverage_25', 'movingAverage_50']\n",
    "    df = merged_data[selected_columns]\n",
    "    \n",
    "    df.set_index('securityId', inplace=True)\n",
    "    \n",
    "    # Preprocess the data using .loc to avoid SettingWithCopyWarning\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].dt.date\n",
    "    df.loc[:, 'signalDate'] = pd.to_datetime(df['signalDate'])\n",
    "    df.loc[:, 'signalDate'] = df['signalDate'].astype(str)    \n",
    "    df = df.sort_values(by='signalDate')\n",
    "    \n",
    "    # Spliting the data into training and validation sets\n",
    "    validation_date = '2024-04-01'\n",
    "    train_df = df[df['signalDate'] < validation_date]\n",
    "    validation_df = df[df['signalDate'] == validation_date]\n",
    "    \n",
    "    # Checking if there are enough samples for training and validation\n",
    "    if len(train_df) < 3 or len(validation_df) < 1:\n",
    "        print(f\"Not enough data for securityId: {security_id}, skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Spliting into features and target\n",
    "    X_train = train_df.drop(columns=['closePrice'])\n",
    "    y_train = train_df['closePrice']\n",
    "    X_validation = validation_df.drop(columns=['closePrice'])\n",
    "    y_validation = validation_df['closePrice']\n",
    "    \n",
    "    X_validation_with_date = X_validation.copy()\n",
    "    X_validation_with_date['signalDate'] = validation_df['signalDate']\n",
    "\n",
    "    # Defining parameter grids for each model\n",
    "    param_grid_lr = {\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    param_grid_dt = {\n",
    "        'base_estimator__max_depth': [10, 20],\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    param_grid_gb = {\n",
    "        'base_estimator__n_estimators': [50, 100],\n",
    "        'base_estimator__learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [10, 50],\n",
    "        'max_samples': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    # Training Bagging Ridge model\n",
    "    grid_lr = GridSearchCV(estimator=BaggingRegressor(base_estimator=Ridge(), random_state=42),\n",
    "                           param_grid=param_grid_lr, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_lr.fit(X_train.drop(columns=['signalDate']), y_train)\n",
    "    \n",
    "    # Bagging Decision Tree model\n",
    "    grid_dt = GridSearchCV(estimator=BaggingRegressor(base_estimator=DecisionTreeRegressor(), random_state=42),\n",
    "                           param_grid=param_grid_dt, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_dt.fit(X_train.drop(columns=['signalDate']), y_train)\n",
    "    \n",
    "    # Bagging Gradient Boosting model\n",
    "    grid_gb = GridSearchCV(estimator=BaggingRegressor(base_estimator=GradientBoostingRegressor(), random_state=42),\n",
    "                           param_grid=param_grid_gb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_gb.fit(X_train.drop(columns=['signalDate']), y_train)\n",
    "\n",
    "    # Predictions with the best models\n",
    "    lr_bagging_predictions = grid_lr.predict(X_validation.drop(columns=['signalDate']))\n",
    "    dt_bagging_predictions = grid_dt.predict(X_validation.drop(columns=['signalDate']))\n",
    "    gb_bagging_predictions = grid_gb.predict(X_validation.drop(columns=['signalDate']))\n",
    "\n",
    "    # Calculating RMSE for the best models\n",
    "    lr_bagging_rmse = calculate_rmse(y_validation, lr_bagging_predictions)\n",
    "    dt_bagging_rmse = calculate_rmse(y_validation, dt_bagging_predictions)\n",
    "    gb_bagging_rmse = calculate_rmse(y_validation, gb_bagging_predictions)\n",
    "\n",
    "    # Percentage error for the best models\n",
    "    lr_bagging_percentage_error = calculate_percentage_error(y_validation, lr_bagging_predictions)\n",
    "    dt_bagging_percentage_error = calculate_percentage_error(y_validation, dt_bagging_predictions)\n",
    "    gb_bagging_percentage_error = calculate_percentage_error(y_validation, gb_bagging_predictions)\n",
    "\n",
    "    results = {\n",
    "        'securityId': security_id,\n",
    "        'Bagging_LR_RMSE': lr_bagging_rmse,\n",
    "        'Bagging_DT_RMSE': dt_bagging_rmse,\n",
    "        'Bagging_GB_RMSE': gb_bagging_rmse,\n",
    "        'Bagging_LR_Percentage_Error': lr_bagging_percentage_error.mean(),\n",
    "        'Bagging_DT_Percentage_Error': dt_bagging_percentage_error.mean(),\n",
    "        'Bagging_GB_Percentage_Error': gb_bagging_percentage_error.mean()\n",
    "    }\n",
    "    all_results.append(results)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71136ca-329f-46d0-a33f-1b9408925314",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://intelliinvest:intelliinvest@67.211.219.52:27017/intelliinvest')\n",
    "db = client['intelliinvest']\n",
    "collection = db['STOCK_PRICE_FORECAST_2']\n",
    "\n",
    "security_ids = collection.distinct('securityId')\n",
    "\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14eefb0-5090-4275-b4b2-3aa9d2a6277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each securityId\n",
    "for security_id in security_ids:\n",
    "    query = {\"securityId\": security_id}\n",
    "    results = collection.find(query).limit(5000)\n",
    "    \n",
    "    df = pd.DataFrame(list(results))\n",
    "    \n",
    "    # Renaming columns and preprocess dates\n",
    "    df = df.rename(columns={'todayDate': 'signalDate'})\n",
    "    df['signalDate'] = pd.to_datetime(df['signalDate'])\n",
    "    df['tomorrowForecastDate'] = pd.to_datetime(df['tomorrowForecastDate'])\n",
    "    df['signalDate'] = df['signalDate'].dt.date\n",
    "    df['tomorrowForecastDate'] = df['tomorrowForecastDate'].dt.date\n",
    "    df['signalDate'] = pd.to_datetime(df['signalDate'])\n",
    "    \n",
    "    # Filter for rows where the 'signalDate' is April 1, 2024\n",
    "    april_1_2024_data = df[df['signalDate'] == pd.to_datetime('2024-04-01')]\n",
    "    \n",
    "    if not april_1_2024_data.empty:\n",
    "        selected_data = april_1_2024_data[['tomorrowForecastDate', 'tomorrowActualPrice', 'tomorrowForecastPrice', 'securityId']]\n",
    "        all_data.append(selected_data)\n",
    "\n",
    "# Combine all the data into a single DataFrame\n",
    "all_data_df = pd.concat(all_data, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d09138-602d-4ae6-99e1-16212b9954a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns in all_data_df to match results_df\n",
    "all_data_df = all_data_df.rename(columns={\n",
    "    'tomorrowForecastDate': 'signalDate',\n",
    "    'tomorrowActualPrice': 'Actual',\n",
    "    'tomorrowForecastPrice': 'ForecastPrice',\n",
    "    'securityId': 'securityId'\n",
    "})\n",
    "\n",
    "results_df = results_df.rename(columns={\n",
    "    'signalDate': 'signalDate', \n",
    "    'Actual': 'Actual',\n",
    "    'securityId': 'securityId'\n",
    "})\n",
    "\n",
    "# Merge the dataframes on 'securityId' and 'Actual'\n",
    "merged_data = pd.merge(all_data_df, results_df, on=['signalDate', 'Actual', 'securityId'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337bbdb3-1033-4dda-81dc-072232027808",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b6ddb-9f79-4dde-8d5a-b4a9191c9d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://intelliinvest:intelliinvest@67.211.219.52:27017/intelliinvest')\n",
    "db = client['intelliinvest']\n",
    "\n",
    "# Create a new collection \n",
    "collection = db['Stock_forecast_Challenger']\n",
    "\n",
    "merged_data_dict = merged_data.to_dict('records')\n",
    "\n",
    "# Inserting merged_data into the new collection\n",
    "collection.insert_many(merged_data_dict)\n",
    "\n",
    "print(\"Data inserted successfully into 'Stock_forecast_Challenger' collection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180376d7-20b1-4fb3-bcbd-461c9c99a55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99693d7e-0750-4038-ac16-87c96efe99fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
